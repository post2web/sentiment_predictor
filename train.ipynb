{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with an RNN (recurrent neural network)\n",
    "\n",
    "Using an RNN allow us to use the information about the \"sequence\" for the input words and is more accurate. \n",
    "\n",
    "The example data for the current setup is for sentiment. Sentiment analysis is basicly classification with two classes. The model can easily be extended to classify any number of classes.\n",
    "\n",
    "The example is trained over dataset of movie reviews with labels.\n",
    "\n",
    "<img src=\"graph_diagram.png\" width=400px>\n",
    "\n",
    "The embedding layer is used for more efficient representation for our input data than one-hot encoded vectors. The embedding can be pre-trained with word2vec for even better results. But it's good enough to just have an embedding layer and let the network learn the embedding table on it's own for the example data.\n",
    "\n",
    "From the embedding layer, the new representations will be passed to LSTM cells. These will add recurrent connections to the network so we can include information about the sequence of words in the data.\n",
    "\n",
    "Finally, the LSTM cells will go to a fully connected output layer.\n",
    "\n",
    "All outputs from FC layer are ignored except for the very last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from time import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the preprocessed data\n",
    "Need to run preprocess.ipynb first for preprocession the example dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = np.load('data/X_train.npy'), np.load('data/y_train.npy')\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=.9)\n",
    "\n",
    "word_to_id = pickle.load(open('data/word_to_id.p', 'rb'))\n",
    "\n",
    "def get_batches(x, y, batch_size=100):\n",
    "    n_batches = len(x) // batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for i in range(0, len(x), batch_size):\n",
    "        yield x[i:i+batch_size], y[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_size = 256\n",
    "lstm_layers = 1\n",
    "batch_size = 500\n",
    "learning_rate = 0.001\n",
    "embed_size = 300\n",
    "epochs = 50\n",
    "max_epochs_without_improvement = 5\n",
    "\n",
    "checkpoints_dir = 'checkpoints'\n",
    "\n",
    "try:\n",
    "    os.makedirs(checkpoints_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "n_words = len(word_to_id) + 1 # Adding 1 because we use 0's for padding, dictionary started at 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# inputs\n",
    "inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
    "labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "# embedding layer\n",
    "embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1))\n",
    "embed = tf.nn.embedding_lookup(embedding, inputs)\n",
    "\n",
    "# basic LSTM cell with dropout\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "lstm = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "\n",
    "# Stack up multiple LSTM layers, for deep learning\n",
    "cell = tf.contrib.rnn.MultiRNNCell([lstm] * lstm_layers)\n",
    "\n",
    "# initial state of all zeros\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "# run the data through the RNN nodes\n",
    "outputs, final_state = tf.nn.dynamic_rnn(\n",
    "    cell, embed, initial_state=initial_state)\n",
    "\n",
    "# name the state tensors\n",
    "initial_state = tf.identity(initial_state, 'initial_state')\n",
    "final_state = tf.identity(final_state, 'final_state')\n",
    "\n",
    "# output\n",
    "# only care about the final output\n",
    "logits = tf.contrib.layers.fully_connected(\n",
    "    outputs[:, -1],\n",
    "    num_outputs=y.shape[-1],\n",
    "    activation_fn=None\n",
    ")\n",
    "\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "\n",
    "# cost\n",
    "cost = tf.losses.mean_squared_error(labels, logits)\n",
    "\n",
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(labels, 1), tf.argmax(logits, 1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "accuracy = tf.identity(accuracy, 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch:', 0, 'Step:', 5, 'Train loss:', 0.3617655, 'time for 1 step', 0.16616296768188477)\n",
      "('Epoch:', 0, 'Step:', 10, 'Train loss:', 0.34606618, 'time for 1 step', 0.16635513305664062)\n",
      "('Epoch:', 0, 'Step:', 15, 'Train loss:', 0.31412193, 'time for 1 step', 0.18479394912719727)\n",
      "('Epoch:', 0, 'Step:', 20, 'Train loss:', 0.31515089, 'time for 1 step', 0.16063284873962402)\n",
      "('Epoch:', 0, 'Step:', 25, 'Train loss:', 0.28616226, 'time for 1 step', 0.16037607192993164)\n",
      "('Validation accuracy:', 0.59549999)\n",
      "Best model found. Saving ...\n",
      "('Epoch:', 0, 'Step:', 30, 'Train loss:', 0.27816761, 'time for 1 step', 0.15356898307800293)\n",
      "('Epoch:', 0, 'Step:', 35, 'Train loss:', 0.26705703, 'time for 1 step', 0.15573596954345703)\n",
      "('Epoch:', 0, 'Step:', 40, 'Train loss:', 0.25758621, 'time for 1 step', 0.19456696510314941)\n",
      "('Epoch:', 1, 'Step:', 45, 'Train loss:', 0.23615013, 'time for 1 step', 0.15041303634643555)\n",
      "('Epoch:', 1, 'Step:', 50, 'Train loss:', 0.22171262, 'time for 1 step', 0.18409180641174316)\n",
      "('Validation accuracy:', 0.67449999)\n",
      "Best model found. Saving ...\n",
      "('Epoch:', 1, 'Step:', 55, 'Train loss:', 0.21596548, 'time for 1 step', 0.17198705673217773)\n",
      "('Epoch:', 1, 'Step:', 60, 'Train loss:', 0.26184881, 'time for 1 step', 0.15779995918273926)\n",
      "('Epoch:', 1, 'Step:', 65, 'Train loss:', 0.24332497, 'time for 1 step', 0.16866183280944824)\n",
      "('Epoch:', 1, 'Step:', 70, 'Train loss:', 0.24556579, 'time for 1 step', 0.1599900722503662)\n",
      "('Epoch:', 1, 'Step:', 75, 'Train loss:', 0.24185804, 'time for 1 step', 0.15365004539489746)\n",
      "('Validation accuracy:', 0.6415)\n",
      "('Epoch:', 1, 'Step:', 80, 'Train loss:', 0.23218589, 'time for 1 step', 0.15515398979187012)\n",
      "('Epoch:', 2, 'Step:', 85, 'Train loss:', 0.22659744, 'time for 1 step', 0.1540830135345459)\n",
      "('Epoch:', 2, 'Step:', 90, 'Train loss:', 0.22016369, 'time for 1 step', 0.16647696495056152)\n",
      "('Epoch:', 2, 'Step:', 95, 'Train loss:', 0.21673198, 'time for 1 step', 0.1623690128326416)\n",
      "('Epoch:', 2, 'Step:', 100, 'Train loss:', 0.21524625, 'time for 1 step', 0.19264793395996094)\n",
      "('Validation accuracy:', 0.69949996)\n",
      "Best model found. Saving ...\n",
      "('Epoch:', 2, 'Step:', 105, 'Train loss:', 0.19007453, 'time for 1 step', 0.1802990436553955)\n",
      "('Epoch:', 2, 'Step:', 110, 'Train loss:', 0.17514081, 'time for 1 step', 0.16469097137451172)\n",
      "('Epoch:', 2, 'Step:', 115, 'Train loss:', 0.16160682, 'time for 1 step', 0.16607403755187988)\n",
      "('Epoch:', 2, 'Step:', 120, 'Train loss:', 0.17152749, 'time for 1 step', 0.16348814964294434)\n",
      "('Epoch:', 3, 'Step:', 125, 'Train loss:', 0.14936979, 'time for 1 step', 0.15369892120361328)\n",
      "('Validation accuracy:', 0.78299999)\n",
      "Best model found. Saving ...\n",
      "('Epoch:', 3, 'Step:', 130, 'Train loss:', 0.14072388, 'time for 1 step', 0.15862298011779785)\n",
      "('Epoch:', 3, 'Step:', 135, 'Train loss:', 0.13244344, 'time for 1 step', 0.15575289726257324)\n",
      "('Epoch:', 3, 'Step:', 140, 'Train loss:', 0.14727032, 'time for 1 step', 0.18190908432006836)\n",
      "('Epoch:', 3, 'Step:', 145, 'Train loss:', 0.12445534, 'time for 1 step', 0.16342806816101074)\n",
      "('Epoch:', 3, 'Step:', 150, 'Train loss:', 0.11477182, 'time for 1 step', 0.16240715980529785)\n",
      "('Validation accuracy:', 0.7585001)\n",
      "('Epoch:', 3, 'Step:', 155, 'Train loss:', 0.11339772, 'time for 1 step', 0.18823599815368652)\n",
      "('Epoch:', 3, 'Step:', 160, 'Train loss:', 0.14331295, 'time for 1 step', 0.1660139560699463)\n",
      "('Epoch:', 4, 'Step:', 165, 'Train loss:', 0.16400619, 'time for 1 step', 0.15567898750305176)\n",
      "('Epoch:', 4, 'Step:', 170, 'Train loss:', 0.1140293, 'time for 1 step', 0.16173005104064941)\n",
      "('Epoch:', 4, 'Step:', 175, 'Train loss:', 0.10012535, 'time for 1 step', 0.15749502182006836)\n",
      "('Validation accuracy:', 0.8075)\n",
      "Best model found. Saving ...\n",
      "('Epoch:', 4, 'Step:', 180, 'Train loss:', 0.11742787, 'time for 1 step', 0.1611161231994629)\n",
      "('Epoch:', 4, 'Step:', 185, 'Train loss:', 0.10343847, 'time for 1 step', 0.16822195053100586)\n",
      "('Epoch:', 4, 'Step:', 190, 'Train loss:', 0.10694636, 'time for 1 step', 0.1577751636505127)\n",
      "('Epoch:', 4, 'Step:', 195, 'Train loss:', 0.13471092, 'time for 1 step', 0.15459895133972168)\n",
      "('Epoch:', 4, 'Step:', 200, 'Train loss:', 0.10155458, 'time for 1 step', 0.15175914764404297)\n",
      "('Validation accuracy:', 0.8125)\n",
      "Best model found. Saving ...\n",
      "('Epoch:', 5, 'Step:', 205, 'Train loss:', 0.11155149, 'time for 1 step', 0.15733981132507324)\n",
      "('Epoch:', 5, 'Step:', 210, 'Train loss:', 0.10847418, 'time for 1 step', 0.17138981819152832)\n",
      "('Epoch:', 5, 'Step:', 215, 'Train loss:', 0.095950045, 'time for 1 step', 0.15318512916564941)\n",
      "('Epoch:', 5, 'Step:', 220, 'Train loss:', 0.093506128, 'time for 1 step', 0.1600480079650879)\n",
      "('Epoch:', 5, 'Step:', 225, 'Train loss:', 0.097718097, 'time for 1 step', 0.15928411483764648)\n",
      "('Validation accuracy:', 0.80800009)\n",
      "('Epoch:', 5, 'Step:', 230, 'Train loss:', 0.072108522, 'time for 1 step', 0.1622450351715088)\n",
      "('Epoch:', 5, 'Step:', 235, 'Train loss:', 0.068308324, 'time for 1 step', 0.16675519943237305)\n",
      "('Epoch:', 5, 'Step:', 240, 'Train loss:', 0.090114608, 'time for 1 step', 0.19170308113098145)\n",
      "('Epoch:', 6, 'Step:', 245, 'Train loss:', 0.080888584, 'time for 1 step', 0.15730810165405273)\n",
      "('Epoch:', 6, 'Step:', 250, 'Train loss:', 0.12283783, 'time for 1 step', 0.151824951171875)\n",
      "('Validation accuracy:', 0.75650001)\n",
      "('Epoch:', 6, 'Step:', 255, 'Train loss:', 0.11027113, 'time for 1 step', 0.15818190574645996)\n",
      "('Epoch:', 6, 'Step:', 260, 'Train loss:', 0.09221445, 'time for 1 step', 0.16872191429138184)\n",
      "('Epoch:', 6, 'Step:', 265, 'Train loss:', 0.076145023, 'time for 1 step', 0.1565399169921875)\n",
      "('Epoch:', 6, 'Step:', 270, 'Train loss:', 0.078858301, 'time for 1 step', 0.15155601501464844)\n",
      "('Epoch:', 6, 'Step:', 275, 'Train loss:', 0.10312805, 'time for 1 step', 0.19121289253234863)\n",
      "('Validation accuracy:', 0.80900007)\n",
      "('Epoch:', 6, 'Step:', 280, 'Train loss:', 0.092371076, 'time for 1 step', 0.16859793663024902)\n",
      "('Epoch:', 7, 'Step:', 285, 'Train loss:', 0.094118528, 'time for 1 step', 0.1767110824584961)\n",
      "('Epoch:', 7, 'Step:', 290, 'Train loss:', 0.08087071, 'time for 1 step', 0.1688230037689209)\n",
      "('Epoch:', 7, 'Step:', 295, 'Train loss:', 0.069763742, 'time for 1 step', 0.16713905334472656)\n",
      "('Epoch:', 7, 'Step:', 300, 'Train loss:', 0.073928952, 'time for 1 step', 0.15820908546447754)\n",
      "('Validation accuracy:', 0.82900006)\n",
      "Best model found. Saving ...\n",
      "('Epoch:', 7, 'Step:', 305, 'Train loss:', 0.055497997, 'time for 1 step', 0.1566789150238037)\n",
      "('Epoch:', 7, 'Step:', 310, 'Train loss:', 0.05874249, 'time for 1 step', 0.16279292106628418)\n",
      "('Epoch:', 7, 'Step:', 315, 'Train loss:', 0.079960249, 'time for 1 step', 0.16896986961364746)\n",
      "('Epoch:', 7, 'Step:', 320, 'Train loss:', 0.091519989, 'time for 1 step', 0.15900492668151855)\n",
      "('Epoch:', 8, 'Step:', 325, 'Train loss:', 0.069030873, 'time for 1 step', 0.15380597114562988)\n",
      "('Validation accuracy:', 0.82050002)\n",
      "('Epoch:', 8, 'Step:', 330, 'Train loss:', 0.068846606, 'time for 1 step', 0.1672370433807373)\n",
      "('Epoch:', 8, 'Step:', 335, 'Train loss:', 0.057703011, 'time for 1 step', 0.15583014488220215)\n",
      "('Epoch:', 8, 'Step:', 340, 'Train loss:', 0.057946049, 'time for 1 step', 0.15504002571105957)\n",
      "('Epoch:', 8, 'Step:', 345, 'Train loss:', 0.045696113, 'time for 1 step', 0.1558990478515625)\n",
      "('Epoch:', 8, 'Step:', 350, 'Train loss:', 0.061898779, 'time for 1 step', 0.17167997360229492)\n",
      "('Validation accuracy:', 0.833)\n",
      "Best model found. Saving ...\n",
      "('Epoch:', 8, 'Step:', 355, 'Train loss:', 0.056148175, 'time for 1 step', 0.16051506996154785)\n",
      "('Epoch:', 8, 'Step:', 360, 'Train loss:', 0.077932924, 'time for 1 step', 0.20434188842773438)\n",
      "('Epoch:', 9, 'Step:', 365, 'Train loss:', 0.075097784, 'time for 1 step', 0.15375399589538574)\n",
      "('Epoch:', 9, 'Step:', 370, 'Train loss:', 0.080140479, 'time for 1 step', 0.16180109977722168)\n",
      "('Epoch:', 9, 'Step:', 375, 'Train loss:', 0.084162876, 'time for 1 step', 0.15185785293579102)\n",
      "('Validation accuracy:', 0.78349996)\n",
      "('Epoch:', 9, 'Step:', 380, 'Train loss:', 0.082895599, 'time for 1 step', 0.15167903900146484)\n",
      "('Epoch:', 9, 'Step:', 385, 'Train loss:', 0.055860985, 'time for 1 step', 0.15596699714660645)\n",
      "('Epoch:', 9, 'Step:', 390, 'Train loss:', 0.047461338, 'time for 1 step', 0.15900421142578125)\n",
      "('Epoch:', 9, 'Step:', 395, 'Train loss:', 0.04852337, 'time for 1 step', 0.16489291191101074)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch:', 9, 'Step:', 400, 'Train loss:', 0.042710666, 'time for 1 step', 0.1577320098876953)\n",
      "('Validation accuracy:', 0.83099997)\n",
      "('Epoch:', 10, 'Step:', 405, 'Train loss:', 0.047190767, 'time for 1 step', 0.16694307327270508)\n",
      "('Epoch:', 10, 'Step:', 410, 'Train loss:', 0.043301947, 'time for 1 step', 0.1621711254119873)\n",
      "('Epoch:', 10, 'Step:', 415, 'Train loss:', 0.041268766, 'time for 1 step', 0.1515331268310547)\n",
      "('Epoch:', 10, 'Step:', 420, 'Train loss:', 0.047427244, 'time for 1 step', 0.15753483772277832)\n",
      "('Epoch:', 10, 'Step:', 425, 'Train loss:', 0.034123875, 'time for 1 step', 0.1969301700592041)\n",
      "('Validation accuracy:', 0.80899996)\n",
      "('Epoch:', 10, 'Step:', 430, 'Train loss:', 0.039133232, 'time for 1 step', 0.16880488395690918)\n",
      "('Epoch:', 10, 'Step:', 435, 'Train loss:', 0.052304257, 'time for 1 step', 0.15459513664245605)\n",
      "('Epoch:', 10, 'Step:', 440, 'Train loss:', 0.036825299, 'time for 1 step', 0.16773104667663574)\n",
      "('Epoch:', 11, 'Step:', 445, 'Train loss:', 0.037221536, 'time for 1 step', 0.16612601280212402)\n",
      "('Epoch:', 11, 'Step:', 450, 'Train loss:', 0.03276043, 'time for 1 step', 0.1498098373413086)\n",
      "('Validation accuracy:', 0.83350003)\n",
      "Best model found. Saving ...\n",
      "('Epoch:', 11, 'Step:', 455, 'Train loss:', 0.033670679, 'time for 1 step', 0.15005087852478027)\n",
      "('Epoch:', 11, 'Step:', 460, 'Train loss:', 0.05572864, 'time for 1 step', 0.1729140281677246)\n",
      "('Epoch:', 11, 'Step:', 465, 'Train loss:', 0.043046135, 'time for 1 step', 0.16707205772399902)\n",
      "('Epoch:', 11, 'Step:', 470, 'Train loss:', 0.04093587, 'time for 1 step', 0.1624279022216797)\n",
      "('Epoch:', 11, 'Step:', 475, 'Train loss:', 0.038984802, 'time for 1 step', 0.17471003532409668)\n",
      "('Validation accuracy:', 0.75799996)\n",
      "('Epoch:', 11, 'Step:', 480, 'Train loss:', 0.041640189, 'time for 1 step', 0.18869304656982422)\n",
      "('Epoch:', 12, 'Step:', 485, 'Train loss:', 0.055257849, 'time for 1 step', 0.15825796127319336)\n",
      "('Epoch:', 12, 'Step:', 490, 'Train loss:', 0.048146844, 'time for 1 step', 0.15975284576416016)\n",
      "('Epoch:', 12, 'Step:', 495, 'Train loss:', 0.04554807, 'time for 1 step', 0.1550891399383545)\n",
      "('Epoch:', 12, 'Step:', 500, 'Train loss:', 0.07247822, 'time for 1 step', 0.15069818496704102)\n",
      "('Validation accuracy:', 0.80549997)\n",
      "('Epoch:', 12, 'Step:', 505, 'Train loss:', 0.088496268, 'time for 1 step', 0.16783905029296875)\n",
      "('Epoch:', 12, 'Step:', 510, 'Train loss:', 0.088503003, 'time for 1 step', 0.15206408500671387)\n",
      "('Epoch:', 12, 'Step:', 515, 'Train loss:', 0.045594543, 'time for 1 step', 0.16957592964172363)\n",
      "('Epoch:', 12, 'Step:', 520, 'Train loss:', 0.026896955, 'time for 1 step', 0.15155792236328125)\n",
      "('Epoch:', 13, 'Step:', 525, 'Train loss:', 0.038236164, 'time for 1 step', 0.15691709518432617)\n",
      "('Validation accuracy:', 0.82300001)\n",
      "('Epoch:', 13, 'Step:', 530, 'Train loss:', 0.030174177, 'time for 1 step', 0.15081286430358887)\n",
      "('Epoch:', 13, 'Step:', 535, 'Train loss:', 0.029063698, 'time for 1 step', 0.16586995124816895)\n",
      "('Epoch:', 13, 'Step:', 540, 'Train loss:', 0.036384553, 'time for 1 step', 0.15446996688842773)\n",
      "('Epoch:', 13, 'Step:', 545, 'Train loss:', 0.026726833, 'time for 1 step', 0.15675806999206543)\n",
      "('Epoch:', 13, 'Step:', 550, 'Train loss:', 0.035690419, 'time for 1 step', 0.16910195350646973)\n",
      "('Validation accuracy:', 0.82449996)\n",
      "('Epoch:', 13, 'Step:', 555, 'Train loss:', 0.03343812, 'time for 1 step', 0.1781320571899414)\n",
      "('Epoch:', 13, 'Step:', 560, 'Train loss:', 0.023297802, 'time for 1 step', 0.169281005859375)\n",
      "('Epoch:', 14, 'Step:', 565, 'Train loss:', 0.032999683, 'time for 1 step', 0.185089111328125)\n",
      "('Epoch:', 14, 'Step:', 570, 'Train loss:', 0.022215685, 'time for 1 step', 0.1578540802001953)\n",
      "('Epoch:', 14, 'Step:', 575, 'Train loss:', 0.027631989, 'time for 1 step', 0.16933989524841309)\n",
      "('Validation accuracy:', 0.82249999)\n",
      "Trining done.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "# enable JIT optimizer\n",
    "config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "best_accuracy = -np.inf\n",
    "epochs_without_improvement = 0\n",
    "step = 0\n",
    "for epoch in range(epochs):\n",
    "    state = sess.run(initial_state)\n",
    "    \n",
    "    if epochs_without_improvement >= max_epochs_without_improvement:\n",
    "        break\n",
    "\n",
    "    for x, y in get_batches(X_train, y_train, batch_size):\n",
    "        step += 1\n",
    "        feed = {inputs: x, labels: y, keep_prob: 0.3, initial_state: state}\n",
    "        time_start = time.time()\n",
    "        loss, state, _ = sess.run(\n",
    "            [cost, final_state, optimizer], feed_dict=feed)\n",
    "        step_time = time.time() - time_start\n",
    "\n",
    "        if step % 5==0:\n",
    "            print(\"Epoch:\", epoch, \"Step:\", step, \"Train loss:\", loss, 'time for 1 step', step_time)\n",
    "\n",
    "        if step % 25==0:\n",
    "            val_acc = []\n",
    "            val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "            for x, y in get_batches(X_valid, y_valid, batch_size):\n",
    "                feed = {inputs: x, labels: y, keep_prob: 1,\n",
    "                        initial_state: val_state}\n",
    "                batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                val_acc.append(batch_acc)\n",
    "            print(\"Validation accuracy:\", np.mean(val_acc))\n",
    "            if np.mean(val_acc) > best_accuracy:\n",
    "                best_accuracy = np.mean(val_acc)\n",
    "                print('Best model found. Saving ...')\n",
    "                saver.save(sess, checkpoints_dir + '/model.ckpt')\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                \n",
    "            if epochs_without_improvement >= max_epochs_without_improvement:\n",
    "                break\n",
    "\n",
    "print('Trining done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
